{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**MODEL LOADING**"
      ],
      "metadata": {
        "id": "o-tyYIyBGGe2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i33cvwAv5sOd"
      },
      "outputs": [],
      "source": [
        "!pip install transformers accelerate bitsandbytes torch\n",
        "!pip install youtube-transcript-api chromadb\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "from chromadb import Client\n",
        "\n",
        "model_name = \"HuggingFaceH4/zephyr-7b-beta\"\n",
        "youtube_video_id = \"J5_-l7WIO_w\"\n",
        "print(f\"Loading model: {model_name}\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        "    load_in_8bit=True\n",
        ")\n",
        "print(\"Model loaded.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Chain Building***"
      ],
      "metadata": {
        "id": "hmqt93LpHCSJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", message=\"MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\")\n",
        "\n",
        "youtube_video_id = \"ukzFI9rgwfU\"\n",
        "\n",
        "\n",
        "# --- Fetch YouTube Transcript ---\n",
        "print(f\"Fetching transcript for video ID: {youtube_video_id}\")\n",
        "try:\n",
        "    transcript_list = YouTubeTranscriptApi.get_transcript(youtube_video_id)\n",
        "    transcript_text = \" \".join([item['text'] for item in transcript_list])\n",
        "    print(\"Transcript fetched successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error fetching transcript: {e}\")\n",
        "    transcript_text = \"\"\n",
        "\n",
        "client = Client()\n",
        "collection_name = f\"youtube_transcript_{youtube_video_id}\"\n",
        "try:\n",
        "    collection = client.get_collection(collection_name)\n",
        "    print(f\"Using existing collection: {collection_name}\")\n",
        "except:\n",
        "    collection = client.create_collection(collection_name)\n",
        "    print(f\"Created new collection: {collection_name}\")\n",
        "\n",
        "if transcript_text:\n",
        "    sentences = transcript_text.split('.')\n",
        "    ids = [f\"chunk_{i}\" for i in range(len(sentences))]\n",
        "    documents = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
        "\n",
        "    if documents:\n",
        "        print(f\"Adding {len(documents)} document chunks to ChromaDB.\")\n",
        "        collection.add(\n",
        "            documents=documents,\n",
        "            ids=ids\n",
        "        )\n",
        "        print(\"Documents added to ChromaDB.\")\n",
        "    else:\n",
        "        print(\"No valid document chunks to add.\")\n",
        "else:\n",
        "    print(\"No transcript available to store in ChromaDB.\")\n",
        "\n",
        "def query_transcript(query_text, n_results=5):\n",
        "    \"\"\"\n",
        "    Queries the ChromaDB collection for relevant chunks and uses the LLM\n",
        "    to generate a response based on the query and retrieved context.\n",
        "    \"\"\"\n",
        "    print(f\"\\nQuerying for: {query_text}\")\n",
        "    results = collection.query(\n",
        "        query_texts=[query_text],\n",
        "        n_results=n_results\n",
        "    )\n",
        "\n",
        "    retrieved_chunks = results['documents'][0]\n",
        "    print(f\"Retrieved {len(retrieved_chunks)} relevant chunks.\")\n",
        "\n",
        "    if not retrieved_chunks:\n",
        "        return \"Could not find relevant information in the transcript.\"\n",
        "    context = \"\\n\".join(retrieved_chunks)\n",
        "    prompt = f\"\"\"Using the following context, answer the question.\n",
        "    If the answer is not in the context, say \"I could not find the answer in the transcript.\"\n",
        "    Context:\n",
        "    {context}\n",
        "    Question: {query_text}\n",
        "\n",
        "    Answer:\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"Generating response with LLM...\")\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=256,\n",
        "            num_return_sequences=1,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            do_sample=True,\n",
        "            top_k=50,\n",
        "            top_p=0.95\n",
        "        )\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    answer_start_index = response.find(\"Answer:\")\n",
        "    if answer_start_index != -1:\n",
        "        generated_answer = response[answer_start_index + len(\"Answer:\"):].strip()\n",
        "    else:\n",
        "        generated_answer = response.strip()\n",
        "    return generated_answer\n",
        "\n",
        "print(\"Training Complete\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prfR-QPl6hNF",
        "outputId": "209ff96d-14a9-4af6-808b-43aa40a9a991"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching transcript for video ID: ukzFI9rgwfU\n",
            "Error fetching transcript: no element found: line 1, column 0\n",
            "Using existing collection: youtube_transcript_ukzFI9rgwfU\n",
            "No transcript available to store in ChromaDB.\n",
            "Training Complete\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_query = \"what is machine learning?\"\n",
        "answer = query_transcript(user_query)\n",
        "print(\"\\nLLM Answer:\")\n",
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gEaC8f5_EOkw",
        "outputId": "19581448-5582-4016-d10f-5bcbb379b418"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Querying for: what is machine learning?\n",
            "Retrieved 1 relevant chunks.\n",
            "Generating response with LLM...\n",
            "\n",
            "LLM Answer:\n",
            "Machine learning is a process by which machines can learn from past data and experiences, make predictions or decisions, and understand and reason based on that learning. It goes beyond just learning, as it involves understanding the relationships between different variables and the outcomes of those variables. Machine learning involves different approaches, such as supervised learning, unsupervised learning, and reinforcement learning, depending on whether the data is labeled or unlabeled and whether feedback or rewards are provided. The availability of large amounts of data, advances in computer memory and processing capabilities, and applications in various industries, such as healthcare, finance, and transportation, make machine learning a rapidly growing field in technology.\n",
            "\n",
            "    Context:\n",
            "    We know humans learn from their past experiences and machines follow instructions given by humans, but what if humans can train machines to learn from past data and do what humans can do, and much faster? That's called machine learning, but it's much more than just learning; it's also about understanding and reasoning. In this video, we will learn the basics of machine learning, so that's Paul (he loves listening to new songs, and he either likes them or dislikes them). Paul decides this based on the song's tempo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j0x0-E4sFGE_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}